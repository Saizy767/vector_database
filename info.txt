ğŸ“˜ Project Overview
====================

Project Name: VectorDB ETL Pipeline
Language: Python 3.10+
Frameworks/Libraries:
- SQLAlchemy â€“ for database connectivity and ORM
- Transformers (Hugging Face) â€“ for text embedding (BERT, Sentence Transformers)
- Pydantic / Pydantic Settings â€“ for configuration validation and management
- Pytest â€“ for testing

This project implements an ETL (Extractâ€“Transformâ€“Load) pipeline that converts textual data from a relational database into vector embeddings and loads the results back into another SQL table. It is designed to support hybrid search systems or vector databases.


ğŸ§© Project Structure
====================
project/
â”‚
â”œâ”€â”€ main.py                     # Entry point: runs the full ETL pipeline
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ settings.py             # Pydantic-based configuration loader (.env support)
â”‚   â”œâ”€â”€ models/                 # SQLAlchemy ORM models
â”‚   â”œâ”€â”€ vectordb/               # Core logic for Vector ETL
â”‚   â”‚   â”œâ”€â”€ connector/          # Database connectors (SQLConnector)
â”‚   â”‚   â”œâ”€â”€ etl/                # Extractors, Transformers, Loaders
â”‚   â”‚   â”œâ”€â”€ embedding/          # Embedding models (BERT, Sentence Transformers)
â”‚   â”‚   â”œâ”€â”€ metadata/           # Metadata builder for chunks
â”‚   â”‚   â”œâ”€â”€ splitters/          # Sentence splitting utilities
â”‚   â”‚   â””â”€â”€ utils.py            # Helper functions (logging, cosine similarity)
â”‚   â””â”€â”€ tests/                  # Comprehensive pytest-based test suite
â”‚
â””â”€â”€ .env                        # Environment variables (not tracked by Git)


âš™ï¸ How It Works
====================
1. Extraction
   Data is read in batches from a source SQL table using SQLExtractor.

2. Transformation
   Each text record is:
   - Split into sentences or smaller chunks using SentenceSpliter
   - Transformed into numerical vectors (embeddings) via:
     - SentenceTransformerEmbedding or
     - BERTEmbedder
   - Enriched with metadata using MetadataBuilder

3. Loading
   The transformed chunks and embeddings are inserted (or upserted) into a target SQL table using SQLLoader.


ğŸ§  Key Components
====================
| Component | Purpose |
|------------|----------|
| SQLConnector | Handles DB connections and sessions |
| SQLExtractor | Reads rows or batches from SQL tables |
| Transformer | Coordinates text splitting, embedding, and metadata |
| SQLLoader | Performs bulk insert or upsert of embeddings |
| SentenceSpliter | Splits raw text into sentences or segments |
| MetadataBuilder | Generates consistent, validated chunk metadata |
| EmbeddingChapter | ORM model for storing chunks and embeddings |


ğŸ§¾ Environment Configuration (.env)
====================
Example .env file:
DB_URL=postgresql+psycopg2://user:password@localhost:5432/mydb
EXTRACT_TABLE_NAME=source_table
LOAD_TABLE_NAME=embedding_chapter
SOURCE_ID=id
EMBEDDING_COLUMNS=chapter_text
METADATA_COLUMNS=chapter_title,chapter_number
EMBEDDING_PROVIDER=bert
EMBEDDING_MODEL=bert-base-uncased
DEVICE=cuda


â–¶ï¸ Running the Pipeline
====================
# Activate environment
python -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Run ETL pipeline
python main.py


ğŸ§ª Running Tests
====================
All modules are tested with pytest:

export PYTHONPATH=$(pwd)/src
pytest src/tests/embedding/ -v --cov=vectordb.embedding
pytest src/tests/connector/ -v --cov=vectordb.connector
pytest src/tests/etl/extractors/ -v --cov=vectordb.etl.extractors
pytest src/tests/etl/loaders/ -v --cov=vectordb.etl.loaders
pytest src/tests/splitters/ -v --cov=vectordb.splitters
pytest src/tests/metadata/ -v --cov=vectordb.metadata


ğŸ“‚ Logging
====================
Logs are saved to app.log and also displayed in the console.
Log format: [timestamp] [level] module: message


ğŸ› ï¸ Extensibility
====================
- Embedding providers â€“ easily extend by implementing new classes inheriting from BaseEmbedding.
- Custom metadata â€“ configure field mappings via .env.
- Additional data sources â€“ implement new connectors based on BaseConnector.